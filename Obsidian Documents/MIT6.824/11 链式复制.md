
2004年论文

# 主题

- zookeeper锁
- 链式复制

# zookeeper lock（zookeeper实现分布式锁）

接上周zookeeper话题。
**使用zookeeper原语+临时+序号节点实现锁。**

## 粗糙实现

粗糙的锁的实现：acquire和release，即lock and unlock。
获取锁在zookeeper上创建临时znode，因此是个写操作，是线性一致性的。

acquir()：
1. 尝试创建一个**临时znode节点(ephemeral)（如果客户端断开连接，自动删除）**，如果存在则返回（表示获取到锁）
2. 如果znode节点已存在exist，则阻塞wait等待(这里通过watch监听znode变更事件)，直到znode消失/被删除后，重新发起create一个znode请求（尝试获取锁）
release()：
1. 获取锁后，解锁只需要delete之前创建的临时znode即可。
伪代码：
```cpp
acquire() :
	while true:
		if create(ephermenal):
			break;
		if exists('if', watch):
			wait notification
release():
	delete("if")
```
缺点：羊群效应。每次所有上次未获取锁的客户端重新尝试获取锁。

## 更好的做法

论文伪代码：
```cpp
Lock 
// 每个客户端都创建lock-n（lock-0 --- lock-999)，并返回n
1 n = create(l + "/lock-", EPHEMERAL | ***SEQUENTIAL***)  // lock-0/1/2/3
2 C = getChildren(l, false) 
// 如果n是最小的，那么给他锁
3 if n is lowest znode in C, exit 
// watch前一个znode
4 p = znode in C ordered just before n 如果n为10，p为9.
5 if exists(p, true) wait for watch event 
6 goto 2 

Unlock 
1 delete(n)
```
未获取锁的客户端排成一条线，按顺序获取锁。
这种锁也叫标签锁。
获取的锁是zlock。不同于go lock and mutex.
![](Pasted%20image%2020230729175341.png)

## 优缺点

优点：
- 可靠性、有序性有序获取锁避免了死锁、通知机制不需要一直轮询、临时znode会断开自动删除、不受时钟影响
缺点：
- 性能开销大：zk是一个分布式协调服务。每次创建释放锁只能leader来执行。

zlock应用场景：
- **选举(leader election)**：从一堆client中选举一个leader。并且如果有必要，可以让leader清理中间状态(临界区的一些状态值，比如把state清0之类的)。
- **软锁("soft" lock)**：通常执行map只需要一个mapper，可以通过软锁保证mapreduce的worker中一次只有一个mapper接受某个特定的map任务。当然，这里mapeer如果failed，锁就会被释放，然后其他mapper可以尝试获取锁，重新执行这个mapper任务。而对于mapreduce程序来说，重复执行(执行两次)是可接受的，因为函数式编程，相同输入下运行几次都应该有相同的输出结果。即一个操作可以发生两次。

# 链式复制

是主从复制协议的一种

## 两种构造RSM方案

1. run all ops through Raft/Paxos
2. Configuration server. 比如zookeeper服务。配置服务器内部可能使用Paxos、Raft、ZAB等共识算法。配置服务器扮演coordinator或者master（如GFS）。配置服务器除了采用共识算法实现外，还可以运行主备复制。
	- 比如GFS虽然只有一个master，但是chunk服务器却有primary和一堆backups，它们有一个用于主备复制（P/B R）的协议。
	- 而VM-FT中，配置服务器是test-and-set的storage服务器，其决定谁是primary，另一方则是backup，primary和back之间通过channel或其他形式同步log等数据，尽管略有延迟。
2更常见通用。

 **采用配置服务器+主备复制的特点**：
- **复制服务在状态state维护的成本较低，一般需要维护的数据量很少**
- **主备复制则主要负责大量的数据复制工作**

​而直接用共识算法实现复制状态机，需要直接在提供服务的server之间来回进行大量的数据复制，检查点的state数据同步等工作，一般实现会更复杂。

## 概述

链式复制，就是方案2的主备复制方案。
论文中假设有一个配置服务器（论文中称为master）
链式复制特性：
1. 读/查询操作只涉及一个服务器(TAIL)，不是只读primary/leader。
2. simple recovery plan
3. linearizability,**线性一致性。**
![](Pasted%20image%2020230715180037.png)

## 读写操作

客户端直接向tail节点读，然后tail节点直接相应。
写操作客户端向配置服务器找到头节点，然后响头结点发送写请求，头节点服务器生成log等，通过storage存储更新相关state状态，然后传递给下个节点。最终传递到tail节点，完成修改存储后响应client写请求。
tail节点就是提交点。

## Crash 崩溃与恢复

有三种类型：头节点、中间节点、尾节点

配置服务器中记录了谁是头节点谁是尾节点。
当头节点崩溃时，配置服务器根据记录信息进行头节点选举，然后告知。

配置服务器需要重新告知怎么重组链，怎么同步，头为节点分别是谁。

对比Raft论文图7、图8，简单很多

## add replica

流程：

1. 假设原tail节点S2下新增S3节点，此时client依旧和原tail节点S2交互
2. S2会持续将同步信息传递给S3，并且记录哪些log已经同步到S3
3. 某时刻S3和S2同步完成，S3通过配置服务告知S2，自己可以成为新的tail
4. 配置服务设置S3为新tail
5. 后续client改成请求tail节点获取读写响应（client可以通过配置服务知道谁是head、tail）

## CR vs. Raft

CR只涉及主备复制，没有配置服务。

优势：

- CR拆分client RPC请求负载到head和tail：头接受写请求。尾接受读请求，响应写请求。raft全部通过leader。
- 头节点向下个节点发送一次update；raft向所有节点发送log entry。
- 读操作只涉及tail节点；即使实现了只读优化，leader也要将读操作log entry同步到其他节点，确保了大多数才能相应读操作。
- 崩溃恢复更简单。

劣势：

- 一个节点故障就要重新配置，可能会有段时间无法提供服务。因为写入必须通过整个链顺序发送；raft只需要大多数满足write持久化，就可以继续工作

## 并行读优化扩展

​ 由于链式复制中，只需要tail响应read请求，这里可以做一些优化的工作，进一步提高read吞吐量。进行了负载均衡。

​ **基本思路是进行拆分(split)对象，论文中称之为volume，将对象拆分到多个链中(splits object across many chain)**。

- Chain1：S1、S2、S3 (tail)
- Chain2：S2、S3、S1 (tail)
- Chian3：S3、S1、S2 (tail)

​ **可以通过配置服务进行一些数据分片shard操作。如果数据被均匀write到Chain1～Chain3，那么读操作可以并行地命中不同的shards，均匀分布下，读吞吐量(read throughput)会线性增加，理想情况下这里能得到3倍的吞吐量**。

# 总结

方法1 ： lab3.所有操作-配置和复制都是使用raft
方法2：配置服务器（运行raft/paxos） + P/B 使用链式复制

